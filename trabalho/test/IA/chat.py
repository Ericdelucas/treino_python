import random
import nltk
import requests
import json
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Baixa recursos do NLTK (sÃ³ na 1Âª execuÃ§Ã£o)
nltk.download('punkt', quiet=True)

# ğŸ§  Base local simples (respostas diretas)
base_conhecimento = {
    "oi": ["OlÃ¡! Como posso te ajudar?", "Oi! Tudo bem?"],
    "ola": ["OlÃ¡! Tudo certo?", "Oi! Prazer em te ver por aqui!"],
    "bom dia": ["Bom dia â˜€ï¸ Espero que tenha um Ã³timo dia!", "Bom dia! Como posso ajudar?"],
    "boa tarde": ["Boa tarde ğŸŒ Tudo bem contigo?", "Boa tarde! O que deseja saber hoje?"],
    "boa noite": ["Boa noite ğŸŒ™ Como foi o seu dia?", "Boa noite! EstÃ¡ tudo bem por aÃ­?"],
    "tudo bem": ["Tudo Ã³timo! E com vocÃª?", "Estou bem, obrigado por perguntar ğŸ˜„"],
    "quem te criou": ["Fui criado por um desenvolvedor que usa Python, Scikit-Learn e IA local com Ollama!"],
    "qual seu nome": ["Sou o PyBot ğŸ¤–, um assistente hÃ­brido com inteligÃªncia artificial local!"],
    "o que voce faz": ["Posso responder perguntas simples e tambÃ©m gerar textos, resumos ou ideias com IA local."],
    "adeus": ["AtÃ© logo!", "Tchau! Foi bom conversar com vocÃª ğŸ‘‹"],
    "default": ["Desculpe, nÃ£o entendi. Vou pensar melhor sobre isso ğŸ˜…"]
}

# VetorizaÃ§Ã£o para busca semÃ¢ntica local
perguntas = list(base_conhecimento.keys())
respostas = list(base_conhecimento.values())
vectorizer = CountVectorizer()
X_counts = vectorizer.fit_transform(perguntas)
tfidf = TfidfTransformer().fit_transform(X_counts)


def responder_local(texto_usuario):
    """Busca respostas rÃ¡pidas na base local."""
    texto_usuario = texto_usuario.lower()
    entrada_counts = vectorizer.transform([texto_usuario])
    entrada_tfidf = TfidfTransformer().fit_transform(entrada_counts)
    similaridades = cosine_similarity(entrada_tfidf, tfidf)
    indice = similaridades.argmax()

    if similaridades.max() < 0.35:
        return None
    else:
        return random.choice(respostas[indice])


def responder_ia(texto_usuario):
    """Usa modelo local do Ollama (respostas longas e inteligentes)."""
    try:
        # ParÃ¢metros otimizados para textos complexos e maiores
        payload = {
            "model": "llama3",              # ou "phi3" se quiser leve
            "prompt": texto_usuario,
            "stream": True,                 # Ativa modo streaming
            "options": {
                "temperature": 0.7,         # Criatividade moderada
                "num_predict": 1024,        # Permite respostas longas (~4k tokens)
                "top_p": 0.9                # Controle de diversidade
            }
        }

        resposta = ""
        with requests.post("http://localhost:11434/api/generate", json=payload, stream=True, timeout=300) as resp:
            for line in resp.iter_lines():
                if line:
                    try:
                        data = json.loads(line.decode("utf-8"))
                        if "response" in data:
                            print(data["response"], end="", flush=True)
                            resposta += data["response"]
                    except json.JSONDecodeError:
                        pass
        print()
        return resposta.strip() or "ğŸ¤” NÃ£o consegui formular uma resposta agora."

    except requests.exceptions.ConnectionError:
        return "âš ï¸ O Ollama nÃ£o estÃ¡ rodando. Inicie com: ollama serve"
    except Exception as e:
        return f"âš ï¸ Erro ao gerar resposta: {e}"


# ğŸ’¬ Loop principal
print("ğŸ¤– Chat HÃ­brido IA (Modo Local - Ollama)\nDigite 'sair' para encerrar.\n")

while True:
    user_input = input("VocÃª: ")
    if user_input.lower() in ["sair", "exit", "quit"]:
        print("PyBot: AtÃ© mais! ğŸ‘‹")
        break

    resposta_local = responder_local(user_input)

    if resposta_local:
        print("PyBot:", resposta_local)
    else:
        print("PyBot (IA pensando... ğŸ¤”)")
        resposta_gerada = responder_ia(user_input)
        print("\nPyBot:", resposta_gerada)
